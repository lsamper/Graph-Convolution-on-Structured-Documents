{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model that does learn on cora dataset\n",
    "\n",
    "[Cora dataset](https://github.com/tkipf/keras-gcn/tree/master/kegra/data/cora):\n",
    "- nodes are papers\n",
    "- node features : words (bag of words),  1433 unique words.\n",
    "- adjacency matrix: $a_{ij} = 1$ if paper $i$ cites paper $j$\n",
    "- label: on of the following classes: \n",
    "    - Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, inforcement_Learning, Rule_Learning, Theory.\n",
    "\n",
    "\n",
    "https://github.com/tkipf/keras-gcn/blob/master/kegra/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import *\n",
    "\n",
    "import time\n",
    "\n",
    "# Define parameters\n",
    "DATASET = 'cora'\n",
    "FILTER = 'localpool'  # 'chebyshev'\n",
    "MAX_DEGREE = 2  # maximum polynomial degree\n",
    "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "\n",
    "# Get data\n",
    "X, A, y = load_data(path=\"../data/cora/\",dataset=DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local pooling filters...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize X\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "\n",
    "if FILTER == 'localpool':\n",
    "    \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
    "    print('Using local pooling filters...')\n",
    "    A_ = preprocess_adj(A, SYM_NORM)\n",
    "    support = 1\n",
    "    graph = [X, A_]\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n",
    "\n",
    "X_in = Input(shape=(X.shape[1],))\n",
    "\n",
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(rate=0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(rate=0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1433)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1433)         0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_3 (GraphConvo (None, 16)           22944       dropout_3[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           graph_convolution_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_4 (GraphConvo (None, 7)            119         dropout_4[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,063\n",
      "Trainable params: 23,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9359 train_acc= 0.2929 val_loss= 1.9368 val_acc= 0.3467 time= 0.9949\n",
      "Epoch: 0002 train_loss= 1.9240 train_acc= 0.3000 val_loss= 1.9261 val_acc= 0.3533 time= 0.0632\n",
      "Epoch: 0003 train_loss= 1.9110 train_acc= 0.3143 val_loss= 1.9147 val_acc= 0.3533 time= 0.0746\n",
      "Epoch: 0004 train_loss= 1.8971 train_acc= 0.3000 val_loss= 1.9024 val_acc= 0.3533 time= 0.0677\n",
      "Epoch: 0005 train_loss= 1.8828 train_acc= 0.3000 val_loss= 1.8898 val_acc= 0.3500 time= 0.0722\n",
      "Epoch: 0006 train_loss= 1.8677 train_acc= 0.3000 val_loss= 1.8768 val_acc= 0.3500 time= 0.0688\n",
      "Epoch: 0007 train_loss= 1.8526 train_acc= 0.3071 val_loss= 1.8640 val_acc= 0.3533 time= 0.0805\n",
      "Epoch: 0008 train_loss= 1.8377 train_acc= 0.3071 val_loss= 1.8514 val_acc= 0.3567 time= 0.0879\n",
      "Epoch: 0009 train_loss= 1.8228 train_acc= 0.3143 val_loss= 1.8389 val_acc= 0.3567 time= 0.0776\n",
      "Epoch: 0010 train_loss= 1.8082 train_acc= 0.3429 val_loss= 1.8268 val_acc= 0.3567 time= 0.0843\n",
      "Epoch: 0011 train_loss= 1.7940 train_acc= 0.3500 val_loss= 1.8152 val_acc= 0.3567 time= 0.0704\n",
      "Epoch: 0012 train_loss= 1.7803 train_acc= 0.3500 val_loss= 1.8040 val_acc= 0.3567 time= 0.0623\n",
      "Epoch: 0013 train_loss= 1.7674 train_acc= 0.3429 val_loss= 1.7935 val_acc= 0.3567 time= 0.0643\n",
      "Epoch: 0014 train_loss= 1.7552 train_acc= 0.3429 val_loss= 1.7839 val_acc= 0.3567 time= 0.0763\n",
      "Epoch: 0015 train_loss= 1.7438 train_acc= 0.3429 val_loss= 1.7752 val_acc= 0.3567 time= 0.0618\n",
      "Epoch: 0016 train_loss= 1.7330 train_acc= 0.3429 val_loss= 1.7671 val_acc= 0.3567 time= 0.0619\n",
      "Epoch: 0017 train_loss= 1.7226 train_acc= 0.3571 val_loss= 1.7597 val_acc= 0.3567 time= 0.0628\n",
      "Epoch: 0018 train_loss= 1.7124 train_acc= 0.3571 val_loss= 1.7525 val_acc= 0.3600 time= 0.0693\n",
      "Epoch: 0019 train_loss= 1.7023 train_acc= 0.3643 val_loss= 1.7454 val_acc= 0.3600 time= 0.0654\n",
      "Epoch: 0020 train_loss= 1.6921 train_acc= 0.3643 val_loss= 1.7384 val_acc= 0.3600 time= 0.0633\n",
      "Epoch: 0021 train_loss= 1.6818 train_acc= 0.3714 val_loss= 1.7313 val_acc= 0.3600 time= 0.0799\n",
      "Epoch: 0022 train_loss= 1.6712 train_acc= 0.3714 val_loss= 1.7240 val_acc= 0.3633 time= 0.0846\n",
      "Epoch: 0023 train_loss= 1.6603 train_acc= 0.3714 val_loss= 1.7164 val_acc= 0.3633 time= 0.0824\n",
      "Epoch: 0024 train_loss= 1.6493 train_acc= 0.3857 val_loss= 1.7089 val_acc= 0.3633 time= 0.0872\n",
      "Epoch: 0025 train_loss= 1.6381 train_acc= 0.3929 val_loss= 1.7013 val_acc= 0.3667 time= 0.0646\n",
      "Epoch: 0026 train_loss= 1.6269 train_acc= 0.3929 val_loss= 1.6936 val_acc= 0.3667 time= 0.0641\n",
      "Epoch: 0027 train_loss= 1.6154 train_acc= 0.4000 val_loss= 1.6856 val_acc= 0.3700 time= 0.0648\n",
      "Epoch: 0028 train_loss= 1.6041 train_acc= 0.4071 val_loss= 1.6779 val_acc= 0.3700 time= 0.0736\n",
      "Epoch: 0029 train_loss= 1.5930 train_acc= 0.4214 val_loss= 1.6704 val_acc= 0.3700 time= 0.0619\n",
      "Epoch: 0030 train_loss= 1.5821 train_acc= 0.4286 val_loss= 1.6633 val_acc= 0.3767 time= 0.0676\n",
      "Epoch: 0031 train_loss= 1.5713 train_acc= 0.4429 val_loss= 1.6563 val_acc= 0.3833 time= 0.0625\n",
      "Epoch: 0032 train_loss= 1.5604 train_acc= 0.4429 val_loss= 1.6495 val_acc= 0.4100 time= 0.0678\n",
      "Epoch: 0033 train_loss= 1.5494 train_acc= 0.4571 val_loss= 1.6424 val_acc= 0.4167 time= 0.0619\n",
      "Epoch: 0034 train_loss= 1.5379 train_acc= 0.4571 val_loss= 1.6349 val_acc= 0.4300 time= 0.0616\n",
      "Epoch: 0035 train_loss= 1.5262 train_acc= 0.4929 val_loss= 1.6270 val_acc= 0.4333 time= 0.0617\n",
      "Epoch: 0036 train_loss= 1.5143 train_acc= 0.5071 val_loss= 1.6188 val_acc= 0.4367 time= 0.0819\n",
      "Epoch: 0037 train_loss= 1.5022 train_acc= 0.5143 val_loss= 1.6102 val_acc= 0.4367 time= 0.0668\n",
      "Epoch: 0038 train_loss= 1.4900 train_acc= 0.5143 val_loss= 1.6013 val_acc= 0.4400 time= 0.0630\n",
      "Epoch: 0039 train_loss= 1.4779 train_acc= 0.5143 val_loss= 1.5925 val_acc= 0.4400 time= 0.0653\n",
      "Epoch: 0040 train_loss= 1.4659 train_acc= 0.5286 val_loss= 1.5840 val_acc= 0.4433 time= 0.0718\n",
      "Epoch: 0041 train_loss= 1.4541 train_acc= 0.5429 val_loss= 1.5754 val_acc= 0.4533 time= 0.0612\n",
      "Epoch: 0042 train_loss= 1.4424 train_acc= 0.5500 val_loss= 1.5669 val_acc= 0.4567 time= 0.0770\n",
      "Epoch: 0043 train_loss= 1.4309 train_acc= 0.5500 val_loss= 1.5586 val_acc= 0.4633 time= 0.0862\n",
      "Epoch: 0044 train_loss= 1.4196 train_acc= 0.5571 val_loss= 1.5503 val_acc= 0.4700 time= 0.0827\n",
      "Epoch: 0045 train_loss= 1.4083 train_acc= 0.5571 val_loss= 1.5421 val_acc= 0.4833 time= 0.0731\n",
      "Epoch: 0046 train_loss= 1.3969 train_acc= 0.5643 val_loss= 1.5341 val_acc= 0.4933 time= 0.0723\n",
      "Epoch: 0047 train_loss= 1.3856 train_acc= 0.5786 val_loss= 1.5260 val_acc= 0.4933 time= 0.0729\n",
      "Epoch: 0048 train_loss= 1.3743 train_acc= 0.5857 val_loss= 1.5179 val_acc= 0.4933 time= 0.0611\n",
      "Epoch: 0049 train_loss= 1.3631 train_acc= 0.6000 val_loss= 1.5096 val_acc= 0.4967 time= 0.0646\n",
      "Epoch: 0050 train_loss= 1.3518 train_acc= 0.6071 val_loss= 1.5012 val_acc= 0.5000 time= 0.0820\n",
      "Epoch: 0051 train_loss= 1.3405 train_acc= 0.6286 val_loss= 1.4929 val_acc= 0.5067 time= 0.0612\n",
      "Epoch: 0052 train_loss= 1.3294 train_acc= 0.6429 val_loss= 1.4847 val_acc= 0.5167 time= 0.0628\n",
      "Epoch: 0053 train_loss= 1.3184 train_acc= 0.6500 val_loss= 1.4768 val_acc= 0.5300 time= 0.0686\n",
      "Epoch: 0054 train_loss= 1.3075 train_acc= 0.6714 val_loss= 1.4688 val_acc= 0.5433 time= 0.0799\n",
      "Epoch: 0055 train_loss= 1.2965 train_acc= 0.6786 val_loss= 1.4605 val_acc= 0.5500 time= 0.0912\n",
      "Epoch: 0056 train_loss= 1.2856 train_acc= 0.6929 val_loss= 1.4521 val_acc= 0.5567 time= 0.0841\n",
      "Epoch: 0057 train_loss= 1.2749 train_acc= 0.6929 val_loss= 1.4438 val_acc= 0.5600 time= 0.0989\n",
      "Epoch: 0058 train_loss= 1.2643 train_acc= 0.6929 val_loss= 1.4358 val_acc= 0.5733 time= 0.0640\n",
      "Epoch: 0059 train_loss= 1.2538 train_acc= 0.7143 val_loss= 1.4281 val_acc= 0.5767 time= 0.0636\n",
      "Epoch: 0060 train_loss= 1.2435 train_acc= 0.7143 val_loss= 1.4205 val_acc= 0.5867 time= 0.0613\n",
      "Epoch: 0061 train_loss= 1.2334 train_acc= 0.7214 val_loss= 1.4129 val_acc= 0.6033 time= 0.0758\n",
      "Epoch: 0062 train_loss= 1.2234 train_acc= 0.7357 val_loss= 1.4052 val_acc= 0.6167 time= 0.0681\n",
      "Epoch: 0063 train_loss= 1.2134 train_acc= 0.7429 val_loss= 1.3973 val_acc= 0.6267 time= 0.0670\n",
      "Epoch: 0064 train_loss= 1.2034 train_acc= 0.7500 val_loss= 1.3892 val_acc= 0.6300 time= 0.0678\n",
      "Epoch: 0065 train_loss= 1.1933 train_acc= 0.7500 val_loss= 1.3809 val_acc= 0.6300 time= 0.0749\n",
      "Epoch: 0066 train_loss= 1.1831 train_acc= 0.7500 val_loss= 1.3727 val_acc= 0.6300 time= 0.0607\n",
      "Epoch: 0067 train_loss= 1.1730 train_acc= 0.7500 val_loss= 1.3645 val_acc= 0.6267 time= 0.0623\n",
      "Epoch: 0068 train_loss= 1.1631 train_acc= 0.7500 val_loss= 1.3568 val_acc= 0.6300 time= 0.0729\n",
      "Epoch: 0069 train_loss= 1.1533 train_acc= 0.7500 val_loss= 1.3492 val_acc= 0.6300 time= 0.0623\n",
      "Epoch: 0070 train_loss= 1.1437 train_acc= 0.7500 val_loss= 1.3417 val_acc= 0.6333 time= 0.0623\n",
      "Epoch: 0071 train_loss= 1.1344 train_acc= 0.7500 val_loss= 1.3344 val_acc= 0.6367 time= 0.0622\n",
      "Epoch: 0072 train_loss= 1.1251 train_acc= 0.7571 val_loss= 1.3272 val_acc= 0.6433 time= 0.0672\n",
      "Epoch: 0073 train_loss= 1.1161 train_acc= 0.7571 val_loss= 1.3201 val_acc= 0.6500 time= 0.0635\n",
      "Epoch: 0074 train_loss= 1.1074 train_acc= 0.7571 val_loss= 1.3130 val_acc= 0.6567 time= 0.0615\n",
      "Epoch: 0075 train_loss= 1.0989 train_acc= 0.7571 val_loss= 1.3060 val_acc= 0.6633 time= 0.0617\n",
      "Epoch: 0076 train_loss= 1.0903 train_acc= 0.7643 val_loss= 1.2987 val_acc= 0.6667 time= 0.0827\n",
      "Epoch: 0077 train_loss= 1.0817 train_acc= 0.7643 val_loss= 1.2917 val_acc= 0.6733 time= 0.0700\n",
      "Epoch: 0078 train_loss= 1.0729 train_acc= 0.7786 val_loss= 1.2848 val_acc= 0.6733 time= 0.0607\n",
      "Epoch: 0079 train_loss= 1.0644 train_acc= 0.7857 val_loss= 1.2782 val_acc= 0.6800 time= 0.0807\n",
      "Epoch: 0080 train_loss= 1.0561 train_acc= 0.7857 val_loss= 1.2716 val_acc= 0.6900 time= 0.0915\n",
      "Epoch: 0081 train_loss= 1.0479 train_acc= 0.7857 val_loss= 1.2649 val_acc= 0.6900 time= 0.0860\n",
      "Epoch: 0082 train_loss= 1.0400 train_acc= 0.7857 val_loss= 1.2583 val_acc= 0.6833 time= 0.0882\n",
      "Epoch: 0083 train_loss= 1.0322 train_acc= 0.7929 val_loss= 1.2519 val_acc= 0.6833 time= 0.0672\n",
      "Epoch: 0084 train_loss= 1.0243 train_acc= 0.8000 val_loss= 1.2456 val_acc= 0.6867 time= 0.0632\n",
      "Epoch: 0085 train_loss= 1.0165 train_acc= 0.8071 val_loss= 1.2394 val_acc= 0.6900 time= 0.0613\n",
      "Epoch: 0086 train_loss= 1.0089 train_acc= 0.8071 val_loss= 1.2334 val_acc= 0.6933 time= 0.0698\n",
      "Epoch: 0087 train_loss= 1.0016 train_acc= 0.8143 val_loss= 1.2275 val_acc= 0.6933 time= 0.0661\n",
      "Epoch: 0088 train_loss= 0.9946 train_acc= 0.8143 val_loss= 1.2219 val_acc= 0.6867 time= 0.0668\n",
      "Epoch: 0089 train_loss= 0.9881 train_acc= 0.8143 val_loss= 1.2166 val_acc= 0.6867 time= 0.0648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0090 train_loss= 0.9812 train_acc= 0.8143 val_loss= 1.2114 val_acc= 0.6867 time= 0.0813\n",
      "Epoch: 0091 train_loss= 0.9739 train_acc= 0.8143 val_loss= 1.2067 val_acc= 0.6900 time= 0.0885\n",
      "Epoch: 0092 train_loss= 0.9668 train_acc= 0.8143 val_loss= 1.2024 val_acc= 0.7000 time= 0.0842\n",
      "Epoch: 0093 train_loss= 0.9602 train_acc= 0.8214 val_loss= 1.1986 val_acc= 0.7000 time= 0.0888\n",
      "Epoch: 0094 train_loss= 0.9540 train_acc= 0.8214 val_loss= 1.1949 val_acc= 0.6967 time= 0.0806\n",
      "Epoch: 0095 train_loss= 0.9474 train_acc= 0.8214 val_loss= 1.1900 val_acc= 0.7033 time= 0.0700\n",
      "Epoch: 0096 train_loss= 0.9408 train_acc= 0.8214 val_loss= 1.1847 val_acc= 0.7033 time= 0.0716\n",
      "Epoch: 0097 train_loss= 0.9340 train_acc= 0.8143 val_loss= 1.1786 val_acc= 0.7033 time= 0.0664\n",
      "Epoch: 0098 train_loss= 0.9272 train_acc= 0.8143 val_loss= 1.1719 val_acc= 0.7000 time= 0.0607\n",
      "Epoch: 0099 train_loss= 0.9209 train_acc= 0.8143 val_loss= 1.1652 val_acc= 0.6967 time= 0.0625\n",
      "Epoch: 0100 train_loss= 0.9149 train_acc= 0.8143 val_loss= 1.1589 val_acc= 0.7033 time= 0.0695\n",
      "Epoch: 0101 train_loss= 0.9088 train_acc= 0.8214 val_loss= 1.1532 val_acc= 0.7000 time= 0.0665\n",
      "Epoch: 0102 train_loss= 0.9029 train_acc= 0.8214 val_loss= 1.1476 val_acc= 0.7033 time= 0.0625\n",
      "Epoch: 0103 train_loss= 0.8972 train_acc= 0.8214 val_loss= 1.1423 val_acc= 0.7067 time= 0.0677\n",
      "Epoch: 0104 train_loss= 0.8913 train_acc= 0.8214 val_loss= 1.1372 val_acc= 0.7067 time= 0.0819\n",
      "Epoch: 0105 train_loss= 0.8850 train_acc= 0.8214 val_loss= 1.1326 val_acc= 0.7133 time= 0.0676\n",
      "Epoch: 0106 train_loss= 0.8785 train_acc= 0.8214 val_loss= 1.1291 val_acc= 0.7100 time= 0.0607\n",
      "Epoch: 0107 train_loss= 0.8728 train_acc= 0.8214 val_loss= 1.1262 val_acc= 0.7133 time= 0.0639\n",
      "Epoch: 0108 train_loss= 0.8677 train_acc= 0.8500 val_loss= 1.1239 val_acc= 0.7200 time= 0.0823\n",
      "Epoch: 0109 train_loss= 0.8630 train_acc= 0.8500 val_loss= 1.1215 val_acc= 0.7233 time= 0.0694\n",
      "Epoch: 0110 train_loss= 0.8578 train_acc= 0.8500 val_loss= 1.1182 val_acc= 0.7267 time= 0.0612\n",
      "Epoch: 0111 train_loss= 0.8518 train_acc= 0.8500 val_loss= 1.1129 val_acc= 0.7267 time= 0.0639\n",
      "Epoch: 0112 train_loss= 0.8459 train_acc= 0.8571 val_loss= 1.1071 val_acc= 0.7233 time= 0.0759\n",
      "Epoch: 0113 train_loss= 0.8405 train_acc= 0.8357 val_loss= 1.1019 val_acc= 0.7200 time= 0.0633\n",
      "Epoch: 0114 train_loss= 0.8361 train_acc= 0.8357 val_loss= 1.0976 val_acc= 0.7167 time= 0.0678\n",
      "Epoch: 0115 train_loss= 0.8315 train_acc= 0.8357 val_loss= 1.0941 val_acc= 0.7200 time= 0.0706\n",
      "Epoch: 0116 train_loss= 0.8265 train_acc= 0.8357 val_loss= 1.0906 val_acc= 0.7200 time= 0.0677\n",
      "Epoch: 0117 train_loss= 0.8214 train_acc= 0.8357 val_loss= 1.0872 val_acc= 0.7200 time= 0.0636\n",
      "Epoch: 0118 train_loss= 0.8160 train_acc= 0.8357 val_loss= 1.0838 val_acc= 0.7233 time= 0.0616\n",
      "Epoch: 0119 train_loss= 0.8106 train_acc= 0.8429 val_loss= 1.0807 val_acc= 0.7267 time= 0.0698\n",
      "Epoch: 0120 train_loss= 0.8058 train_acc= 0.8571 val_loss= 1.0772 val_acc= 0.7267 time= 0.0628\n",
      "Epoch: 0121 train_loss= 0.8017 train_acc= 0.8571 val_loss= 1.0742 val_acc= 0.7300 time= 0.0619\n",
      "Epoch: 0122 train_loss= 0.7975 train_acc= 0.8571 val_loss= 1.0711 val_acc= 0.7200 time= 0.0670\n",
      "Epoch: 0123 train_loss= 0.7924 train_acc= 0.8571 val_loss= 1.0670 val_acc= 0.7233 time= 0.0728\n",
      "Epoch: 0124 train_loss= 0.7868 train_acc= 0.8571 val_loss= 1.0618 val_acc= 0.7267 time= 0.0734\n",
      "Epoch: 0125 train_loss= 0.7815 train_acc= 0.8643 val_loss= 1.0573 val_acc= 0.7300 time= 0.0627\n",
      "Epoch: 0126 train_loss= 0.7761 train_acc= 0.8643 val_loss= 1.0527 val_acc= 0.7333 time= 0.0635\n",
      "Epoch: 0127 train_loss= 0.7711 train_acc= 0.8643 val_loss= 1.0487 val_acc= 0.7367 time= 0.0754\n",
      "Epoch: 0128 train_loss= 0.7662 train_acc= 0.8714 val_loss= 1.0444 val_acc= 0.7400 time= 0.0967\n",
      "Epoch: 0129 train_loss= 0.7617 train_acc= 0.8714 val_loss= 1.0399 val_acc= 0.7467 time= 0.0861\n",
      "Epoch: 0130 train_loss= 0.7577 train_acc= 0.8714 val_loss= 1.0360 val_acc= 0.7467 time= 0.0818\n",
      "Epoch: 0131 train_loss= 0.7536 train_acc= 0.8714 val_loss= 1.0327 val_acc= 0.7500 time= 0.0805\n",
      "Epoch: 0132 train_loss= 0.7496 train_acc= 0.8714 val_loss= 1.0302 val_acc= 0.7567 time= 0.0703\n",
      "Epoch: 0133 train_loss= 0.7459 train_acc= 0.8714 val_loss= 1.0283 val_acc= 0.7567 time= 0.0656\n",
      "Epoch: 0134 train_loss= 0.7422 train_acc= 0.8714 val_loss= 1.0263 val_acc= 0.7600 time= 0.0651\n",
      "Epoch: 0135 train_loss= 0.7379 train_acc= 0.8714 val_loss= 1.0238 val_acc= 0.7667 time= 0.0752\n",
      "Epoch: 0136 train_loss= 0.7330 train_acc= 0.8786 val_loss= 1.0200 val_acc= 0.7633 time= 0.0821\n",
      "Epoch: 0137 train_loss= 0.7283 train_acc= 0.8929 val_loss= 1.0165 val_acc= 0.7667 time= 0.0784\n",
      "Epoch: 0138 train_loss= 0.7238 train_acc= 0.8929 val_loss= 1.0127 val_acc= 0.7667 time= 0.0823\n",
      "Epoch: 0139 train_loss= 0.7195 train_acc= 0.8929 val_loss= 1.0091 val_acc= 0.7600 time= 0.0706\n",
      "Epoch: 0140 train_loss= 0.7147 train_acc= 0.8929 val_loss= 1.0051 val_acc= 0.7567 time= 0.0673\n",
      "Epoch: 0141 train_loss= 0.7098 train_acc= 0.8929 val_loss= 1.0011 val_acc= 0.7600 time= 0.0611\n",
      "Epoch: 0142 train_loss= 0.7049 train_acc= 0.8929 val_loss= 0.9971 val_acc= 0.7667 time= 0.0710\n",
      "Epoch: 0143 train_loss= 0.7003 train_acc= 0.8929 val_loss= 0.9942 val_acc= 0.7667 time= 0.0681\n",
      "Epoch: 0144 train_loss= 0.6962 train_acc= 0.8929 val_loss= 0.9910 val_acc= 0.7667 time= 0.0605\n",
      "Epoch: 0145 train_loss= 0.6921 train_acc= 0.8929 val_loss= 0.9876 val_acc= 0.7700 time= 0.0648\n",
      "Epoch: 0146 train_loss= 0.6883 train_acc= 0.8929 val_loss= 0.9843 val_acc= 0.7700 time= 0.0752\n",
      "Epoch: 0147 train_loss= 0.6850 train_acc= 0.8929 val_loss= 0.9816 val_acc= 0.7700 time= 0.0616\n",
      "Epoch: 0148 train_loss= 0.6816 train_acc= 0.8929 val_loss= 0.9794 val_acc= 0.7700 time= 0.0643\n",
      "Epoch: 0149 train_loss= 0.6780 train_acc= 0.9143 val_loss= 0.9772 val_acc= 0.7733 time= 0.0661\n",
      "Epoch: 0150 train_loss= 0.6742 train_acc= 0.9214 val_loss= 0.9745 val_acc= 0.7733 time= 0.0743\n",
      "Epoch: 0151 train_loss= 0.6707 train_acc= 0.9214 val_loss= 0.9716 val_acc= 0.7667 time= 0.0631\n",
      "Epoch: 0152 train_loss= 0.6676 train_acc= 0.9214 val_loss= 0.9690 val_acc= 0.7700 time= 0.0617\n",
      "Epoch: 0153 train_loss= 0.6646 train_acc= 0.9286 val_loss= 0.9671 val_acc= 0.7700 time= 0.0616\n",
      "Epoch: 0154 train_loss= 0.6614 train_acc= 0.9286 val_loss= 0.9655 val_acc= 0.7733 time= 0.0730\n",
      "Epoch: 0155 train_loss= 0.6579 train_acc= 0.9286 val_loss= 0.9636 val_acc= 0.7767 time= 0.0693\n",
      "Epoch: 0156 train_loss= 0.6542 train_acc= 0.9286 val_loss= 0.9616 val_acc= 0.7833 time= 0.0632\n",
      "Epoch: 0157 train_loss= 0.6506 train_acc= 0.9286 val_loss= 0.9590 val_acc= 0.7833 time= 0.0682\n",
      "Epoch: 0158 train_loss= 0.6470 train_acc= 0.9286 val_loss= 0.9565 val_acc= 0.7867 time= 0.0654\n",
      "Epoch: 0159 train_loss= 0.6431 train_acc= 0.9286 val_loss= 0.9534 val_acc= 0.7900 time= 0.0635\n",
      "Epoch: 0160 train_loss= 0.6389 train_acc= 0.9286 val_loss= 0.9496 val_acc= 0.7900 time= 0.0667\n",
      "Epoch: 0161 train_loss= 0.6350 train_acc= 0.9286 val_loss= 0.9463 val_acc= 0.7900 time= 0.0642\n",
      "Epoch: 0162 train_loss= 0.6311 train_acc= 0.9286 val_loss= 0.9435 val_acc= 0.7900 time= 0.0708\n",
      "Epoch: 0163 train_loss= 0.6274 train_acc= 0.9286 val_loss= 0.9407 val_acc= 0.7900 time= 0.0687\n",
      "Epoch: 0164 train_loss= 0.6242 train_acc= 0.9214 val_loss= 0.9386 val_acc= 0.7900 time= 0.0609\n",
      "Epoch: 0165 train_loss= 0.6208 train_acc= 0.9214 val_loss= 0.9356 val_acc= 0.7900 time= 0.0646\n",
      "Epoch: 0166 train_loss= 0.6172 train_acc= 0.9286 val_loss= 0.9327 val_acc= 0.7900 time= 0.0740\n",
      "Epoch: 0167 train_loss= 0.6132 train_acc= 0.9357 val_loss= 0.9293 val_acc= 0.7900 time= 0.0631\n",
      "Epoch: 0168 train_loss= 0.6093 train_acc= 0.9357 val_loss= 0.9262 val_acc= 0.7900 time= 0.0633\n",
      "Epoch: 0169 train_loss= 0.6061 train_acc= 0.9357 val_loss= 0.9239 val_acc= 0.7867 time= 0.0622\n",
      "Epoch: 0170 train_loss= 0.6028 train_acc= 0.9357 val_loss= 0.9209 val_acc= 0.7900 time= 0.0676\n",
      "Epoch: 0171 train_loss= 0.5996 train_acc= 0.9357 val_loss= 0.9182 val_acc= 0.7900 time= 0.0645\n",
      "Epoch: 0172 train_loss= 0.5964 train_acc= 0.9357 val_loss= 0.9163 val_acc= 0.7900 time= 0.0621\n",
      "Epoch: 0173 train_loss= 0.5934 train_acc= 0.9357 val_loss= 0.9151 val_acc= 0.7933 time= 0.0685\n",
      "Epoch: 0174 train_loss= 0.5908 train_acc= 0.9357 val_loss= 0.9133 val_acc= 0.7933 time= 0.0707\n",
      "Epoch: 0175 train_loss= 0.5886 train_acc= 0.9357 val_loss= 0.9127 val_acc= 0.7933 time= 0.0635\n",
      "Epoch: 0176 train_loss= 0.5864 train_acc= 0.9357 val_loss= 0.9110 val_acc= 0.7900 time= 0.0620\n",
      "Epoch: 0177 train_loss= 0.5840 train_acc= 0.9357 val_loss= 0.9092 val_acc= 0.7867 time= 0.0626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0178 train_loss= 0.5819 train_acc= 0.9357 val_loss= 0.9073 val_acc= 0.7867 time= 0.0708\n",
      "Epoch: 0179 train_loss= 0.5792 train_acc= 0.9429 val_loss= 0.9054 val_acc= 0.7900 time= 0.0636\n",
      "Epoch: 0180 train_loss= 0.5766 train_acc= 0.9429 val_loss= 0.9039 val_acc= 0.7900 time= 0.0633\n",
      "Epoch: 0181 train_loss= 0.5745 train_acc= 0.9357 val_loss= 0.9034 val_acc= 0.7933 time= 0.0620\n",
      "Epoch: 0182 train_loss= 0.5727 train_acc= 0.9286 val_loss= 0.9031 val_acc= 0.7967 time= 0.0685\n",
      "Epoch: 0183 train_loss= 0.5712 train_acc= 0.9286 val_loss= 0.9025 val_acc= 0.8000 time= 0.0650\n",
      "Epoch: 0184 train_loss= 0.5684 train_acc= 0.9357 val_loss= 0.9007 val_acc= 0.7967 time= 0.0624\n",
      "Epoch: 0185 train_loss= 0.5654 train_acc= 0.9429 val_loss= 0.8979 val_acc= 0.8000 time= 0.0667\n",
      "Epoch: 0186 train_loss= 0.5624 train_acc= 0.9429 val_loss= 0.8956 val_acc= 0.7900 time= 0.0753\n",
      "Epoch: 0187 train_loss= 0.5590 train_acc= 0.9429 val_loss= 0.8920 val_acc= 0.7900 time= 0.0639\n",
      "Epoch: 0188 train_loss= 0.5557 train_acc= 0.9429 val_loss= 0.8883 val_acc= 0.7933 time= 0.0623\n",
      "Epoch: 0189 train_loss= 0.5528 train_acc= 0.9429 val_loss= 0.8847 val_acc= 0.7900 time= 0.0688\n",
      "Epoch: 0190 train_loss= 0.5503 train_acc= 0.9429 val_loss= 0.8814 val_acc= 0.7900 time= 0.0675\n",
      "Epoch: 0191 train_loss= 0.5475 train_acc= 0.9500 val_loss= 0.8785 val_acc= 0.7900 time= 0.0627\n",
      "Epoch: 0192 train_loss= 0.5446 train_acc= 0.9500 val_loss= 0.8764 val_acc= 0.7900 time= 0.0695\n",
      "Epoch: 0193 train_loss= 0.5418 train_acc= 0.9500 val_loss= 0.8747 val_acc= 0.7900 time= 0.0625\n",
      "Epoch: 0194 train_loss= 0.5393 train_acc= 0.9500 val_loss= 0.8735 val_acc= 0.7900 time= 0.0698\n",
      "Epoch: 0195 train_loss= 0.5368 train_acc= 0.9500 val_loss= 0.8725 val_acc= 0.7900 time= 0.0692\n",
      "Epoch: 0196 train_loss= 0.5345 train_acc= 0.9500 val_loss= 0.8728 val_acc= 0.7967 time= 0.0624\n",
      "Epoch: 0197 train_loss= 0.5327 train_acc= 0.9500 val_loss= 0.8734 val_acc= 0.8000 time= 0.0654\n",
      "Epoch: 0198 train_loss= 0.5308 train_acc= 0.9500 val_loss= 0.8731 val_acc= 0.7900 time= 0.0703\n",
      "Epoch: 0199 train_loss= 0.5279 train_acc= 0.9500 val_loss= 0.8701 val_acc= 0.7933 time= 0.0625\n",
      "Epoch: 0200 train_loss= 0.5248 train_acc= 0.9500 val_loss= 0.8665 val_acc= 0.7967 time= 0.0626\n",
      "Test set results: loss= 0.9243 accuracy= 0.7780\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "\n",
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
